{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Oi0Ay2KqJMD",
        "outputId": "e3997031-812c-40dc-e97a-39800ddf789d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-22 19:24:01--  https://s3-ap-southeast-1.amazonaws.com/he-public-data/datasetb2d9982.zip\n",
            "Resolving s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)... 52.219.41.6\n",
            "Connecting to s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)|52.219.41.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 895569552 (854M) [binary/octet-stream]\n",
            "Saving to: ‘datasetb2d9982.zip’\n",
            "\n",
            "datasetb2d9982.zip  100%[===================>] 854.08M  13.8MB/s    in 64s     \n",
            "\n",
            "2023-04-22 19:25:06 (13.3 MB/s) - ‘datasetb2d9982.zip’ saved [895569552/895569552]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-ap-southeast-1.amazonaws.com/he-public-data/datasetb2d9982.zip\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to the ZIP file\n",
        "zip_file_path = 'datasetb2d9982.zip'\n",
        "\n",
        "# Extract the contents of the ZIP file to a directory\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Set the file paths for the extracted dataset\n",
        "base_dir = 'dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'test')\n",
        "sample_submission_dir = os.path.join(base_dir, 'sample_submission')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM-NJgKDvqkP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load the training and testing data into Pandas dataframes\n",
        "train_df = pd.read_csv(\"dataset/train.csv\")\n",
        "test_df = pd.read_csv(\"dataset/test.csv\")\n",
        "\n",
        "# Replace missing or NaN values with empty strings\n",
        "train_df.fillna(\"\", inplace=True)\n",
        "test_df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Convert any integer values to strings in the training data\n",
        "train_df = train_df.applymap(lambda x: str(x) if isinstance(x, int) else x)\n",
        "\n",
        "# Combine the TITLE, DESCRIPTION, and BULLET_POINTS columns into a single list\n",
        "training_sentences = train_df[\"TITLE\"] + train_df[\"DESCRIPTION\"] + train_df[\"BULLET_POINTS\"]\n",
        "testing_sentences = test_df[\"TITLE\"]+ test_df[\"DESCRIPTION\"] + test_df[\"BULLET_POINTS\"]\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "# Convert sentences to sequences of integers using the word index\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_length = 100\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)\n",
        "\n",
        "# Get the word index with the OOV token\n",
        "word_index = tokenizer.word_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJVH_JR-v1o9"
      },
      "outputs": [],
      "source": [
        "# Load the training labels\n",
        "train_labels = train_df['PRODUCT_TYPE_ID'].astype(int)\n",
        "\n",
        "# Align the training labels with the training data\n",
        "training_labels_final = []\n",
        "for i, seq in enumerate(training_sequences):\n",
        "    label = train_labels[i // 3] # Divide by 3 since there are 3 sequences per training example\n",
        "    training_labels_final.append(label)\n",
        "training_labels_final = np.array(training_labels_final)\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(word_index)+1, output_dim=32, input_length=100),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "    tf.keras.layers.Dense(1024, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuxxFLpe6TSO"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foySfU1i6Vsb",
        "outputId": "2324f438-55ba-45a4-dd4f-258f6d121bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3712/3712 [==============================] - 182s 47ms/step - loss: 4008.1765 - mae: 4008.1782 - val_loss: 4003.9719 - val_mae: 4003.9546\n",
            "Epoch 2/10\n",
            "3712/3712 [==============================] - 85s 23ms/step - loss: 4008.1753 - mae: 4008.1711 - val_loss: 4003.9719 - val_mae: 4003.9546\n",
            "Epoch 3/10\n",
            "3712/3712 [==============================] - 83s 22ms/step - loss: 4008.1804 - mae: 4008.1841 - val_loss: 4003.9719 - val_mae: 4003.9546\n",
            "Epoch 4/10\n",
            "3712/3712 [==============================] - 81s 22ms/step - loss: 4008.1758 - mae: 4008.1753 - val_loss: 4003.9719 - val_mae: 4003.9546\n",
            "Epoch 5/10\n",
            "3712/3712 [==============================] - 80s 22ms/step - loss: 4008.1746 - mae: 4008.1753 - val_loss: 4003.9719 - val_mae: 4003.9546\n",
            "Epoch 6/10\n",
            "3712/3712 [==============================] - 79s 21ms/step - loss: 4008.1787 - mae: 4008.1792 - val_loss: 4003.9719 - val_mae: 4003.9546\n",
            "Epoch 7/10\n",
            "3712/3712 [==============================] - 80s 22ms/step - loss: 4008.1858 - mae: 4008.1753 - val_loss: 4003.9719 - val_mae: 4003.9546\n",
            "Epoch 8/10\n",
            "3712/3712 [==============================] - 79s 21ms/step - loss: 4008.1824 - mae: 4008.1802 - val_loss: 4003.9719 - val_mae: 4003.9546\n",
            "Epoch 9/10\n",
            "3712/3712 [==============================] - 79s 21ms/step - loss: 4008.1738 - mae: 4008.1758 - val_loss: 4003.9719 - val_mae: 4003.9546\n",
            "Epoch 10/10\n",
            "3712/3712 [==============================] - 79s 21ms/step - loss: 4008.1758 - mae: 4008.1804 - val_loss: 4003.9719 - val_mae: 4003.9546\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79a13b9c70>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model.fit(training_padded, training_labels_final, epochs=10, batch_size=200, validation_split=0.67)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWkFbU2Lv6_V",
        "outputId": "a7544421-8f92-470a-d9c7-755b23964da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22961/22961 [==============================] - 101s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(testing_padded)\n",
        "y_new=y_pred.tolist()\n",
        "# Create the submission DataFrame\n",
        "submission_df = pd.DataFrame({'PRODUCT_ID': test_df['PRODUCT_ID'], 'PRODUCT_LENGTH': y_new})\n",
        "# Save the submission DataFrame to a CSV file\n",
        "submission_df.to_csv('submission.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}